{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW8lbltHL4pz"
      },
      "source": [
        "# Streamlit으로 인공지능 앱 개발하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcx27IscL9Nq"
      },
      "source": [
        "## Streamlit\n",
        "- Streamlit은 Streamlit 사가 개발한 웹 앱 프레임워크로 데이터 분석 및 기계 학습 코드를 통합한 웹 앱을 간단하게 구축하고 공개할 수 있음\n",
        "- Streamlit의 사이트\n",
        "> URL https://streamlit.io\n",
        "---\n",
        "- Streamlit을 사용해 데이터 분석 결과를 간단히 웹 페이지에 표시 할 수 있음\n",
        "  - 예를 들어 Pandas의 DataFrame을 표로 표시하거나 matplotlib 등으로 작성한 그래프를 삽입 가능\n",
        "  - 간결하고 사용하기 쉬운 UI를 구현할 수 있고 다양한 타입의 앱에 대응 가능\n",
        "- 손쉬운 만틈 반대로 복잡한 앱 개발은 어렵지만 인공지능 모델의 데모를 만들거나 데이터 분석의 결과를 쉽게 멤버들과 공유하고 싶을 떄 매우 편리함\n",
        "- 또한 Streamlit Cloud라는 서비스를 사용하면 GitHub을 경유해 구축한 인공지능 앱을 쉽게 공개할 수 있음\n",
        "  - HTML 코드 작성 필요 X\n",
        "- 개발 예시는 공식 사이트의 갤러리에서 확인 가능\n",
        "> URL https://streamlit.io/gallery\n",
        "  - BERT에 의한 자연어 처리 앱 및 GAN에 의한 얼굴 이미지 생성 앱, 지도를 사용한 앱 등에서 다양한 타입의 인공지능 앱이 소개되어 있음\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0admDJQOZU_"
      },
      "source": [
        "## Streamlit을 이용한 인공지능 앱 개발\n",
        "- 이번 장에선 패션 아이템을 식별하는 인공지능 웹 앱 구축\n",
        "- 패션 아이템의 이미지는 로컬에서 업로드하거나 카메라로 촬영해서 입수함\n",
        "- 그런 다음 이 이미지를 입력해서 훈련한 모델로 예측이 이뤄짐\n",
        "- 예측 결과는 화면에 문장 및 원 그래프로 표시됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbvHttk6Osxw"
      },
      "source": [
        "### 이번 장에서는 다음과 같은 흐름으로 인공지능 앱을 구축하고 공개함\n",
        "1. CNN 모델의 훈련\n",
        "2. Streamlit으로 웹 앱을 구축\n",
        "3. GitHub 경유로 Streamlit Cloud에 앱을 배포\n",
        "---\n",
        "- 먼저 PyTorch에서 CNN 모델을 구축하고 패션 아이템의 이미지를 훈련 데이터로서 사용하여 모델을 훈련함\n",
        "- 글고 훈련한 모델을 사용하는 웹 앱을 프레임워크 Streamlit을 사용하여 구축함\n",
        "- 구축한 앱은 GitHub의 저장소에 업로드하고 Streamlit Cloud와 연계하여 클라우드에 공개함. 이때 URL이 발행되므로 공유해서 많은 사람이 앱을 사용할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9bACdcvPLKE"
      },
      "source": [
        "# 모델 구축과 훈련\n",
        "- Google Colaboratory에서 이미지 식별용의 모델을 구축하고 훈련함\n",
        "- 이번은 Fashion-MNIST를 훈련 데이터로 사용하여 패션 아이템을 식별할 수 있도록 보델을 훈련함\n",
        "- 훈련한 모델은 저장하고 다운로드함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OLMvaHyPYTC"
      },
      "source": [
        "## 훈련 데이터 읽어 들이기와 DAtaLoader의 설정\n",
        "- 훈련 데이터로서 RNN을 이용한 이미지 생성에서 설명한 데이터셋 Fashion-MNIST를 읽어들임\n",
        "  - Fashion-MNIST에는 10개 카테고리, 합계 70000장의 패션 아이템 이미지가 포함되어 있음\n",
        "  - 이미지는 $28 \\times 28$ 픽셀\n",
        "- DataLoader를 데이터 확장과 함께 설정함\n",
        "- 이번에는 배경의 밝기 변동에 대해서도 잘 대응하도록 색 반전도 시행함\n",
        "---\n",
        "* Fashion-MNIST의 원본 데이터는 MIT 라이선스임. MIT 라이선스는 라이선스 표기가 필요하지만 비교적 자유롭게 사용 가능.\n",
        "* 훈련 데이터를 서비스에 이용할 때는 그 데이터의 라이선스에 주의를 기울여야 함\n",
        "- fashion-mnist\n",
        "> URL https://github.com/zalandoresearch/fashion-mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjPKl0MLP0ES",
        "outputId": "6b16931f-9945-47b6-d775-b3bbdcfa7378"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터 읽어들이기와 DataLoader의 설정\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "affine = transforms.RandomAffine((-30, 30), # 회전\n",
        "                                 scale = (0.8, 1.2), # 확대와 축소\n",
        "                                 translate = (0.5, 0.5)) # 이동\n",
        "\n",
        "flip = transforms.RandomHorizontalFlip(p = 0.5) # 좌우 반전\n",
        "invert = transforms.RandomInvert(p = 0.5) # 색의 반전\n",
        "to_tensor = transforms.ToTensor()\n",
        "normalize = transforms.Normalize((0.0), (1.0)) # 평균값을 0, 표준편차를 1로\n",
        "erase = transforms.RandomErasing(p = 0.5) # 일부를 소거\n",
        "\n",
        "transform_train = transforms.Compose([affine, flip, invert, to_tensor, normalize, erase])\n",
        "transform_test = transforms.Compose([to_tensor, normalize])\n",
        "fashion_train = FashionMNIST(\"./data\", train = True, download = True, transform = transform_train)\n",
        "fashion_test = FashionMNIST(\"./data\", train = False, download = True, transform = transform_test)\n",
        "\n",
        "# DataLoader의 설정\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(fashion_train, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(fashion_test, batch_size = batch_size, shuffle = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv1TzRdiSJzM"
      },
      "source": [
        "## 모델 구축\n",
        "- nn.Module() 클래스를 상속받은 클래스로서 CNN 모델을 구축함\n",
        "- 이번엔 <U>배치 정규화(Batch Normalization)을 위한 층인 **nn.BatchNorm2d()** 클래스</U>를 추가함\n",
        "- 배치 정규화는 네트워크 도중에 데이터를 평균 0, 표준편차 1로 변환하여 데이터 분포의 편향을 방지함\n",
        "- 이로 인해 학습이 안정화되고 속도가 빨라짐\n",
        "- 또한, 배치 정규화의 층에는 학습 파라미터가 있으므로 층을 다시 사용할 수 없음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FETXWoJSSp-N",
        "outputId": "89180a19-0d1d-4d96-f7b0-66e7d4b84d67"
      },
      "outputs": [],
      "source": [
        "# 이미지 인식 모델의 구축\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 8, 3)\n",
        "    self.conv2 = nn.Conv2d(8, 16, 3)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.conv3 = nn.Conv2d(16, 32, 3)\n",
        "    self.conv4 = nn.Conv2d(32, 64, 3)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc1 = nn.Linear(64*4*4, 256)\n",
        "    self.dropout = nn.Dropout(p = 0.5)\n",
        "    self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.conv1(x))\n",
        "    x = self.relu(self.bn1(self.conv2(x)))\n",
        "    x = self.pool(x)\n",
        "    x= self.relu(self.conv3(x))\n",
        "    x = self.relu(self.bn2(self.conv4(x)))\n",
        "    x = self.pool(x)\n",
        "    x = x.view(-1, 64*4*4)\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "net.cuda() # GPU 대응\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN_jGAntUuu2"
      },
      "source": [
        "## 학습\n",
        "- 이미지 인식 모델을 훈련함\n",
        "- DataLoader를 사용하여 미니 배치를 꺼내 훈련 및 평가를 실시함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AwbrMPLU0Km",
        "outputId": "f94c8f84-618f-4c8c-83e0-c208426ea0ef"
      },
      "outputs": [],
      "source": [
        "# 이미지 인식 모델의 훈련\n",
        "from torch import optim\n",
        "\n",
        "# 교차 엔트로피 오차 함수\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "# 최적화 알고리즘\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "# 손실 로그\n",
        "record_loss_train = []\n",
        "record_loss_test = []\n",
        "\n",
        "# 학습\n",
        "for i in range(30): # 30 에포크 학습\n",
        "  net.train() # 훈련 모드\n",
        "  loss_train = 0\n",
        "  for j, (x, t) in enumerate(train_loader): # 미니 배치 (x, t)를 꺼낸다\n",
        "    x, t = x.cuda(), t.cuda() # GPU 대응\n",
        "    y = net(x)\n",
        "    loss = loss_fnc(y, t)\n",
        "    loss_train += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  loss_train /= j+1\n",
        "  record_loss_train.append(loss_train)\n",
        "\n",
        "  net.eval() # 평가 모드\n",
        "  loss_test = 0\n",
        "  for j, (x, t) in enumerate(test_loader): # 미니 배치 (x, t)를 꺼낸다\n",
        "    x, t = x.cuda(), t.cuda() # GPU 대응\n",
        "    y = net(x)\n",
        "    loss =loss_fnc(y, t)\n",
        "    loss_test += loss.item()\n",
        "  loss_test /= j+1\n",
        "  record_loss_test.append(loss_test)\n",
        "\n",
        "  if i%1 == 0 :\n",
        "    print(\"Epoch: \", i, \"Loss_Train: \", loss_train, \"Loss_test: \", loss_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbbXcbHsarMp"
      },
      "source": [
        "## 오차 추이\n",
        "- 훈련 데이터와 테스트 데이터 각각 오차 추이를 그래프로 표시함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "0LEjcTvobSiP",
        "outputId": "ab398e0b-4a88-4cf0-e547-4c00baaec9dc"
      },
      "outputs": [],
      "source": [
        "# 오차 추이\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(record_loss_train)), record_loss_train, label = \"Train\")\n",
        "plt.plot(range(len(record_loss_test)), record_loss_test, label = \"Test\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Errors\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhKbxLcabpah"
      },
      "source": [
        "## 정답률\n",
        "- 모델의 성능을 파악하기 위해서 테스트 데이터를 사용해 정답률을 측정함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAc9p46TbuCF",
        "outputId": "0da099e3-8ca7-44b9-ec18-93c23b86ecca"
      },
      "outputs": [],
      "source": [
        "# 정답률의 계산\n",
        "correct = 0\n",
        "total = 0\n",
        "net.eval() # 평가 모드\n",
        "for i, (x, t) in enumerate(test_loader) :\n",
        "  x, t = x.cuda(), t.cuda() # GPU 대응\n",
        "  y = net(x)\n",
        "  correct += (y.argmax(1) == t).sum().item()\n",
        "  total += len(x)\n",
        "\n",
        "print(\"정답률: \", str(correct/total * 100) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaK8pCxQcGYA"
      },
      "source": [
        "## 모델 저장\n",
        "- <U>훈련한 모델의 파라미터를 **state_dict()** 메서드</U>에 의해 취득하고 저장함\n",
        "- 다음 코드는 state_dict() 메서드의 내용을 표시한 후 model_cnn.pth라는 파일명으로 저장함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRdDGryecULW",
        "outputId": "19d44eae-35a3-4857-bced-5c9a1bf9559f"
      },
      "outputs": [],
      "source": [
        "# 모델 저장\n",
        "import torch\n",
        "\n",
        "# state_dict()의 표시\n",
        "for key in net.state_dict():\n",
        "  print(key, \": \", net.state_dict()[key].size())\n",
        "\n",
        "# 저장\n",
        "torch.save(net.state_dict(), \"model_cnn.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpc7WUCWcmiG"
      },
      "source": [
        "## 훈련한 파라미터의 다운로드\n",
        "- 훈련한 모델의 파라미터 model_cnn.pth를 로컬 환경으로 다운로드 해둠\n",
        "- 로컬 환경에 다운로드 해 둔 뒤에, 이 파일을 앱 구축에 이용함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVGWjevedJQW"
      },
      "source": [
        "# 이미지 인식 앱의 구축\n",
        "- Streamlit을 사용하여 이미지를 인식하는 앱을 만듦\n",
        "- 프레임워크로는 PyTorch를 사용하여 원본 CNN 모델을 읽어들여 사용함\n",
        "- 이번은 Google Colaboratory에서 다음 2개의 파일을 만듦\n",
        "  - model.py\n",
        "  - app.py\n",
        "- app.py가 앱의 본체이고, model.py는 훈련한 모델을 하용해 예측을 실시하는 파일임\n",
        "- 이것들을 동작시키기 위해서는 이전에 만든 훈련한 파라미터 model_cnn.pth를 업로드 해야 함\n",
        "- 이번엔 **ngrok**이라는 툴을 사용해 앱의 동작을 확인함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ohLd_XUdkTc"
      },
      "source": [
        "## ngrok의 설정\n",
        "- **ngrok**은 로컬 서버를 외부 공개할 수 있는 톨\n",
        "- 이번은 Google Colaboratory 서버에서 이 ngrok을 사용하여 앱을 공개하고 동작을 확인함\n",
        "- Google Colaboratory에서 ngrok을 사용하려면 ngrok 사이트에 등록하여 **Authtoken**을 취득해야 함\n",
        "- ngrok 웹사이트\n",
        "> URL https://ngrok.com\n",
        "- 로그인 후 인증이 완료되면 ngrok의 대시보드에 도달할 수 있음\n",
        "- 여기서 왼쪽 메뉴의 ** Your Authtoken**을 선택하면 authtoken이 표시되는데 이것을 Copy 버튼을 클릭하여 복사해 둠\n",
        "- 이것은 이후 Authtoken의 설정에서 노트북의 Your Authtoken 부분에 붙여넣게 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3QLXVWpe2gs"
      },
      "source": [
        "## 라이브러리 설치\n",
        "- Streamlit 및 앱의 동작 확인에 사용하는 ngrok을 설치함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VQyEZkz0e0MG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리의 설치\n",
        "%pip install streamlit==1.8.1 --quiet\n",
        "%pip install pyngrok==4.1.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R0CVczCV-t4",
        "outputId": "0ef4e56d-67ec-426c-aab0-cc4c5159ca26"
      },
      "outputs": [],
      "source": [
        "# ngrok 다운로드\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz -O ngrok.tgz\n",
        "\n",
        "# 압축 해제\n",
        "!tar -xvzf ngrok.tgz\n",
        "\n",
        "# ngrok 실행 파일을 이동\n",
        "!mv ngrok /usr/local/bin/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lYU0U-YXNs7",
        "outputId": "1f643b17-4010-4e3d-94da-2b736a172ba3"
      },
      "outputs": [],
      "source": [
        "!ngrok config upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpLAFgnTXWqa",
        "outputId": "b946ddc6-ba1b-4f52-fb40-d0b314cf46d6"
      },
      "outputs": [],
      "source": [
        "!ngrok config check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "bswI8c-Cfeeo",
        "outputId": "98dad19b-68d1-4127-a2b3-c5537e382bcd"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Streamlit과 ngrok을 import한다\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyngrok\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m conf, ngrok\n",
            "File \u001b[1;32mc:\\Users\\jcyjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\streamlit\\__init__.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger \u001b[38;5;28;01mas\u001b[39;00m _logger\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m _config\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRootContainer_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RootContainer\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msecrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Secrets, SECRETS_FILE_LOC\n\u001b[0;32m     51\u001b[0m _LOGGER \u001b[38;5;241m=\u001b[39m _logger\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\jcyjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\streamlit\\proto\\RootContainer_pb2.py:33\u001b[0m\n\u001b[0;32m     12\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m     17\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     18\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamlit/proto/RootContainer.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m   serialized_pb\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#streamlit/proto/RootContainer.proto*&\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mRootContainer\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x08\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124mMAIN\u001b[39m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x07\u001b[39;00m\u001b[38;5;124mSIDEBAR\u001b[39m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m _ROOTCONTAINER \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mEnumDescriptor(\n\u001b[0;32m     27\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRootContainer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRootContainer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     31\u001b[0m   create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[0;32m     32\u001b[0m   values\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 33\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnumValueDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAIN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcreate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_create_key\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     38\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[0;32m     39\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSIDEBAR\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     40\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[0;32m     43\u001b[0m   ],\n\u001b[0;32m     44\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m39\u001b[39m,\n\u001b[0;32m     47\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m77\u001b[39m,\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     49\u001b[0m _sym_db\u001b[38;5;241m.\u001b[39mRegisterEnumDescriptor(_ROOTCONTAINER)\n\u001b[0;32m     51\u001b[0m RootContainer \u001b[38;5;241m=\u001b[39m enum_type_wrapper\u001b[38;5;241m.\u001b[39mEnumTypeWrapper(_ROOTCONTAINER)\n",
            "File \u001b[1;32mc:\\Users\\jcyjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\descriptor.py:796\u001b[0m, in \u001b[0;36mEnumValueDescriptor.__new__\u001b[1;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, index, number,\n\u001b[0;32m    794\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m    795\u001b[0m             options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 796\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m   \u001b[38;5;66;03m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[0;32m    798\u001b[0m   \u001b[38;5;66;03m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[0;32m    799\u001b[0m   \u001b[38;5;66;03m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[0;32m    800\u001b[0m   \u001b[38;5;66;03m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[0;32m    801\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ],
      "source": [
        "# Streamlit과 ngrok을 import한다\n",
        "import streamlit as st\n",
        "from pyngrok import conf, ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aLfDpvtfnpm"
      },
      "source": [
        "## 훈련한 파라미터를 업로드\n",
        "- 훈련한 파라미터 model_cnn.pth를 업로드함\n",
        "- 페이지 왼쪽의 파일의 아이콘을 클릭하고, 표시된 메뉴에서 조금 전에 받아둔 model_cnn.pth를 선택하여 열린 영역으로 드래그&드롭함\n",
        "  - 이로 인해 model_cnn.pth가 Google Colaboratory의 서버에 업로드되고, 노트북에서 읽어 들일 수 있게 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-v_LIsf4u2"
      },
      "source": [
        "## 모델을 다루는 파일\n",
        "- 이미지 인식을 훈련한 모델을 읽어 들이고, 예측을 하는 코드를 model.py에 써넣는다.\n",
        "- 다음 코드의 앞에 있는 **%%writefile**은 매직 커맨드의 일종으로, 지정한 파일에 셀의 내용을 적어 넣음. 이 경우는 이 행 이후의 코드가 파일 model.py에 쓰임\n",
        "- **predict()** 함수는 <U>인수로 img를 받는데</U> 이것은 PIL(Pillow)의 Image형임.\n",
        "- 이것은 훈련한 모델에 맞게 흑백으로 변환하고 크기도 변환함\n",
        "- 나머지는 Tensor로 변환한 다음에 이것을 훈련한 모델에 입력함\n",
        "- 그리고 예측 결과를 조정하여 반환값으로 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BOH-M8iglis",
        "outputId": "13404ce3-be5a-4919-9374-26fdc117baac"
      },
      "outputs": [],
      "source": [
        "# 모델을 다루는 파일 model.py\n",
        "\n",
        "%%writefile model.py\n",
        "# 이하를 model.py에 써넣기\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "classes_kr = [\"셔츠/톱\", \"바지\", \"풀오버\", \"드레스\", \"코드\", \"샌들\", \"와이셔츠\", \"스니커즈\", \"가방\", \"앵클부츠\"]\n",
        "classes_en = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankel boot\"]\n",
        "n_class = len(classes_kr)\n",
        "img_size = 28\n",
        "\n",
        "# 이미지 인식의 모델\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 8, 3)\n",
        "    self.conv2 = nn.Conv2d(8, 16, 3)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.conv3 = nn.Conv2d(16, 32, 3)\n",
        "    self.conv4 = nn.Conv2d(32, 64, 3)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc1 = nn.Linear(64*4*4, 256)\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "    self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.conv1(x))\n",
        "    x = self.relu(self.bn1(self.conv2(x)))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.relu(self.bn2(self.conv4(x)))\n",
        "    x = self.pool(x)\n",
        "    x = x.view(-1, 64*4*4)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# 훈련한 파라미터 읽어 들이기와 설정\n",
        "net.load_state_dict(torch.load(\"model_cnn.pth\", map_location = torch.device(\"cpu\")))\n",
        "\n",
        "def predict(img):\n",
        "  # 모델로 입력\n",
        "  img = img.convert(\"L\") # 흑백으로 변환\n",
        "  img = img.resize((img_size, img_size)) # 크기를 변환\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.0), (1.0))])\n",
        "  img = transform(img)\n",
        "  x = img.reshape(1, 1, img_size, img_size)\n",
        "\n",
        "  # 예측\n",
        "  net.eval()\n",
        "  y = net(x)\n",
        "\n",
        "  # 결과를 반환한다\n",
        "  y_prob = torch.nn.functional.softmax(torch.squeeze(y)) # 확률로 나타낸다\n",
        "  sorted_prob, sorted_indices = torch.sort(y_prob, descending = True) # 내림차순으로 정렬\n",
        "  return [(classes_kr[idx], classes_en[idx], prob.item()) for idx, prob in zip(sorted_indices, sorted_prob)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYqrUywkkHpD"
      },
      "source": [
        "## 앱의 코드\n",
        "- 이미지 인식 앱 본체의 코드를 **app.py**에 써넣음\n",
        "- 로컬에서 업로드 혹은 웹 카메라로 촬영한 이미지 파일에 무엇이 찍혀 있는지를 model.py의 predict() 함수를 사용해 판정함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbX1tTSWkWRp",
        "outputId": "dfb54774-5ccc-4be1-9092-2d6c2be06f4e"
      },
      "outputs": [],
      "source": [
        "# 앱 본체의 파일 app.py\n",
        "\n",
        "%%writefile app.py\n",
        "# 이하를 app.py에 써넣기\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from model import predict\n",
        "\n",
        "st.set_option(\"deprecation.showfileUploaderEncoding\", False)\n",
        "\n",
        "st.sidebar.title(\"이미지 인식 앱\")\n",
        "st.sidebar.write(\"원본 이미지 인식 모델을 사용해서 무슨 이미지인지를 판정합니다.\")\n",
        "\n",
        "st.sidear.write(\"\")\n",
        "\n",
        "img_source = st.sidebar.radio(\"이미지 소스를 선택해 주세요.\", (\"이미지를 업로드\", \"카메라로 촬영\"))\n",
        "\n",
        "if img_source == \"이미지를 업로드\":\n",
        "  img_file = st.sidebar.file_uploader(\"이미지를 선택해 주세요.\", type =[\"png\", \"jpg\", \"jpeg\"])\n",
        "elif img_source == \"카메라로 촬영\":\n",
        "  img_file = st.camera_input(\"카메라로 촬영\")\n",
        "\n",
        "if img_file is not None:\n",
        "  with st.spinner(\"측정 중...\"):\n",
        "    img = Image.open(img_file)\n",
        "    st.image(img, caption = \"대상 이미지\", width = 480)\n",
        "    st.write(\"\")\n",
        "\n",
        "    # 예측\n",
        "    results = predict(img)\n",
        "\n",
        "    # 결과 표시\n",
        "    st.subheader(\"판정 결과\")\n",
        "    n_top = 3 # 확률이 높은 순으로 3위까지 반환한다\n",
        "    for result in results[:n_top]:\n",
        "      st.write(str(round(result[2]*100, 2)) + \"%의 확률로\" + result[0] + \"입니다.\")\n",
        "\n",
        "    # 원 그래프 표시\n",
        "    pie_labels = [result[1] for result in results[:n_top]]\n",
        "    pie_labels.append(\"others\") # 기타\n",
        "    pie_probs = [result[2] for result in results[:n_top]]\n",
        "    pie_probs.append(sum([result[2] for result in results[n_top:]]))\n",
        "\n",
        "    # 기타\n",
        "    fig, ax = plt.subplots()\n",
        "    wedgeprops = {\"width\" : 0.3, \"edgecolor\" : \"white\"}\n",
        "    textprops = {\"fontsize\" : 6}\n",
        "      ax.pie(pie_probs, labels = pie_labels, counterclock = False, startangle = 90, textprops = textprops, autopct = \"%.2f\", wedgeprops = wedgeprops)\n",
        "\n",
        "    # 원 그래프\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.sidebar.write(\"\")\n",
        "st.sidebar.write(\"\")\n",
        "\n",
        "st.sidebar.caption(\"\"\"\n",
        "이 앱은 Fashion-MNIST를 훈련 데이터로 사용하고 있습니다. \\n\n",
        "Copyright (c) 2017 Zalando SE \\n\n",
        "Released under the MIT license \\n\n",
        "https://github.com/zalandoresearch/fashion-minist#license\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInxRUEroQ0J"
      },
      "source": [
        "### 이번에 사용한 주요 Streamlit 코드의 설명\n",
        "- **st.title()** 함수로 타이틀을 표시함. sidebar를 끼워서 사이드 바에 표시되게 함\n",
        "- **st.write()**는 다양한 타입의 인수를 화면에 표시할 수 있는 만능 함수임\n",
        "- **st.radio()** 함수는 라디오 버튼을 배치함\n",
        "- **st.file_uploader()** 함수에 의해 사용자가 파일을 업로드 가능한 영역이 배치됨\n",
        "- **st.camera_input()** 함수로 웹 카메라가 실행되어 촬영이 가능하게 됨\n",
        "- **st.image()** 함수로 화면에 이미지를 표시할 수 있음\n",
        "- **st.pyplot()** 함수로 matplotlib 그래프를 표시할 수 있음\n",
        "- 이외 다른 기능들은 공식 문서 참조\n",
        "- Streamlit LIbrary\n",
        "> URL https://docs.streamlit.io/library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDbHZzjepIFT"
      },
      "source": [
        "## Authtoken의 설정\n",
        "- ngrok로 접속하기 위해 필요한 Authtoken을 설정함\n",
        "- 다음 코드에서 YourAuthtoken 부분을 자신의 ngrok의 Authtoken로 바꾸면 됨\n",
        "> !ngrok authtoken YourAuthtoken\n",
        "---\n",
        "- 다음 절에서 GitHub을 사용하는데 실수로 자신의 AuthToken을 GitHub에 업로드하지 않도록 <font color = \"red\">**주의!**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lajq_LmXpi1t",
        "outputId": "62690a0c-5028-443c-fed8-01691cdcacdd"
      },
      "outputs": [],
      "source": [
        "# Authtoken의 설정\n",
        "!ngrok authtoken 2hDy5JJCU1A2tTmgBQ4wJFNptZz_3Rn9Z2Gtit1feVSjiGdvM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-7J-mZzp_GX"
      },
      "source": [
        "## 앱 실행과 동작 확인\n",
        "- streamlit의 run 명령어로 앱을 실행함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRLXjPbaqCtc"
      },
      "outputs": [],
      "source": [
        "# 앱의 실행\n",
        "!streamlit run app.py &>/dev/null& # &>/dev/null&에 의해 출력을 표시하지 않고 백그라운드 작업으로 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT695HZAqYj4"
      },
      "source": [
        "- ngrok 프로세스를 종료한 다음에 새로 포트를 지정하여 접속함\n",
        "- 접속 결과로 URL을 취득할 수 있음\n",
        "- ngrok의 무료 요금제에서는 동시에 1개의 프로세스만 동작시킬 수 있음\n",
        "- 따라서 오류가 발생했다면 **런타임 $\\rightarrow$ 세션 관리**에서 불필요한 Google Colaboratory 세션을 종료하면 됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "5ZhCqtiIqtyk",
        "outputId": "18640bc0-7765-4c53-a3d2-e1aabc4fbce8"
      },
      "outputs": [],
      "source": [
        "# ngrok에 의한 접속\n",
        "ngrok.kill() # 프로세스 종료\n",
        "url = ngrok.connect(port = \"8501\") # 접속"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEDFoE-aa4q"
      },
      "source": [
        "- URL을 표시하고 링크에서 앱이 동작하는지 확인함\n",
        "  Google Colaboratory 서버를 이용한 일시적 공개라는 점 유의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ovma1WuaiyZ"
      },
      "outputs": [],
      "source": [
        "# 앱의 url을 표시\n",
        "print(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJu6GZK1anQ6"
      },
      "source": [
        "- 링크의 URL을 복사하고 브라우저 URL란에 붙여서 http를 https로 변경하고 페이지를 표시\n",
        "- 앱 화면이 표시되는 것을 확인\n",
        "- 그 다음에 적당한 이미지 파일을 업로드하여 결과가 표시되는 것을 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfP12b_a-59"
      },
      "source": [
        "## requirements.txt의 작성\n",
        "- Streamlit Cloud의 서버에서 앱을 작동하기 위해서 requirements.txt를 작성해야 함\n",
        "- 이 파일에서는 필요한 라이브러리의 버전을 지정함\n",
        "- 우선 앱에서 import한 라이브러리의 버전을 확인함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crJUFPq8bH0C",
        "outputId": "13cb9821-d192-4e39-a5d1-3165cbf3cc5b"
      },
      "outputs": [],
      "source": [
        "# 각 라이브러리의 버전을 확인\n",
        "import streamlit\n",
        "import torch\n",
        "import torchvision\n",
        "import PIL\n",
        "import matplotlib\n",
        "\n",
        "print(\"streamlit==\" + streamlit.__version__)\n",
        "print(\"torch==\" + torch.__version__)\n",
        "print(\"torchvision==\" + torchvision.__version__)\n",
        "print(\"Pillow==\" + PIL.__version__)\n",
        "print(\"matplotlib==\" + matplotlib.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8IVTyDbbkNC"
      },
      "source": [
        "- 앞의 내용을 참고하여 각 라이브러리의 올바른 버전을 기술하고 requirements.txt에 저장함\n",
        "- 또 Pillow와 matplotlib은 버전을 기술하지 않음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux3cKj6ma9ze"
      },
      "outputs": [],
      "source": [
        "# requirements.txt의 작성\n",
        "with open(\"requirements.txt\", \"w\") as w:\n",
        "  w.write(\"streamlit==1.8.1\\n\")\n",
        "  w.write(\"torch=2.3.0\\n\") # GPU 대응은 필요하지 않으므로 cu113은 기술하지 않는다\n",
        "  w.write(\"torchvision=0.18.0\\n\") # # GPU 대응은 필요하지 않으므로 cu113은 기술하지 않는다\n",
        "  w.write(\"Pillow\\n\")\n",
        "  w.write(\"matplotlib\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
